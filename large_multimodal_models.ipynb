{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "685aebb7-3d5c-4697-8540-71f415a3c501",
      "metadata": {
        "id": "685aebb7-3d5c-4697-8540-71f415a3c501"
      },
      "source": [
        "# How do *Language* Models Understand Images, Audio, and Video?\n",
        "\n",
        "<img src=\"./media/mmllm.png\" width=600>\n",
        "\n",
        "Language models began initially by, as the name suggests, modeling language using deep learning techniques. The most popular generative models, Large Language Models (LLMs), build on this capability of understanding language to create new content based on some form of input. Through (primarily) scaling the size of language models up, the ability to complete text based on an input has proven itself quite useful, leading to emergent reasoning, back-and-forth chatting, and advanced contextual understanding of text to near human abilities.\n",
        "\n",
        "<img src=\"./media/scaling_laws.png\" width=600>\n",
        "\n",
        "[*Scaling Laws for Neural Language Models*](https://arxiv.org/pdf/2001.08361)\n",
        "\n",
        "While text understanding is incredibly useful, we live in a multi-dimensional world where understanding through text is just a portion of how we as humans process information. Within the digital realm, we still have visual and auditory understanding, taking the form of images (both photographs or even just your screen), videos, and sound (speech / non-speech). As large language models work towards emulating cognitive human behavior in a digital world researchers have been active in providing these additional modalities.\n",
        "\n",
        "<img src=\"./media/llama3_diagram.png\" width=600>\n",
        "\n",
        "[*Llama 3 Technical Report*](https://arxiv.org/pdf/2407.21783)\n",
        "\n",
        "This notebook will cover a high level overview of some of the techniques being used to teach language models image, video, and audio understanding with corresponding open source model examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756531cf-3db6-4629-8b83-c0e1ffce7fab",
      "metadata": {
        "id": "756531cf-3db6-4629-8b83-c0e1ffce7fab"
      },
      "source": [
        "---\n",
        "# Introducing Vision with Image Understanding\n",
        "\n",
        "<img src=\"./media/gpt4_vision.png\" width=500>\n",
        "\n",
        "[*GPT-4 Technical Report*](https://arxiv.org/pdf/2303.08774)\n",
        "\n",
        "Images were the first new modality integrated into LLMs largely due to the maturity of computer vision techniques and the abundance of visual data. By the time large language models emerged, the vision community had already achieved impressive results in image classification, object detection, and even generative imaging (e.g. photorealistic image generation from text). These advancements made visual understanding a natural starting point for multimodal AI, since models could leverage pre-existing image recognition capabilities. Vision is also a primary sense for humans, so enabling AI to see complements its language understanding in a human-like way. Given this potential and the solid foundation of computer vision, it made sense that **image understanding was the first modality** adopted in LLMs for building multimodal systems.\n",
        "\n",
        "### Vision Transformers (ViT)\n",
        "\n",
        "<img src=\"./media/clip_training.png\" width=500>\n",
        "\n",
        "[*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/pdf/2103.00020)\n",
        "\n",
        "The first major attempt to use similar architectures to LLMs was in Vision Transformers (ViT). ViTs apply the transformer to image data by breaking an image into fixed-size patch “tokens.” In experimentation, researchers split images into 16×16 pixel patches, each of which is flattened and linearly projected into an embedding vector. These patch embeddings are then fed to a Transformer encoder just like they had been with words in a sentence, along with positional encodings to retain information about patch locations. Through self-attention, the model learns relationships between patches across the entire image, enabling it to capture global context and detailed spatial relationships that prior convolutional network approaches might miss.\n",
        "\n",
        "The vision transformer can successfully attend to any region of the image from any other, learning long-range dependencies (for instance, relating an object in one corner to a detail in another) proving that a “pure” transformer could excel at vision tasks. The success of ViT showed that transformers can serve as powerful image encoders, making them ideal for pairing with language models in a multimodal system.\n",
        "\n",
        "### CLIP: Aligning Visual and Language Representations\n",
        "\n",
        "<img src=\"./media/clip_training.png\" width=500>\n",
        "\n",
        "[*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/pdf/2103.00020)\n",
        "\n",
        "While ViT deals with image encoding, **CLIP (Contrastive Language-Image Pretraining)** aligns images with text. In short, CLIP is a model that jointly trains an image encoder and a text encoder to produce a shared embedding space for both modalities. It was trained at scale using over **400 million image-text pairs** collected from the internet with an objective to predict which caption from a batch matches which image. Through deep learning, the CLIP model learns to pull together the embeddings of a corresponding image and caption, and push apart embeddings of mismatched pairs. This **contrastive learning** setup teaches CLIP to generate image and text *representations* that are directly comparable.\n",
        "\n",
        "CLIP’s outcome is a system that understands visual concepts in natural language terms, most importantly resulting in zero-shot classification capabilities. After training, you can give CLIP a new image and a set of text labels (for example, names of ImageNet categories like “cat” or “dog”), and it will identify the label whose text embedding is closest to the image embedding without any task-specific fine-tuning. This demonstrated that **natural language supervision can train extremely flexible visual models**, since CLIP can recognize a wide array of image contents through just pure scaled training on real-world captions. In the context of multimodal AI, CLIP became a foundational as its embeddings can be used to bridge vision and language with **zero-shot image understanding**.\n",
        "\n",
        "### Flamingo: From Understanding to Generation\n",
        "\n",
        "<img src=\"./media/flamingo.png\" width=500>\n",
        "\n",
        "[*Flamingo: a Visual Language Model for Few-Shot Learning*](https://arxiv.org/pdf/2204.14198)\n",
        "\n",
        "While representation learning and encoding was a good first step, the next step was to not just understand images but also to **generate language** about images fluidly. **Flamingo** (by DeepMind) is a visual language model that can accept images and generate text in a combined flexible way. Flamingo achieves this by **integrating a pre-trained vision encoder and a pre-trained language model** and adding special **cross-attention layers** between them. These learnable cross-attention modules  allow the frozen language model to \"understand\" and attend to the image embeddings at various points in its layers, connecting visual information into the text generation architecture. Thus, Flamingo can generat text **conditioning the LM on the content of images** without needing to retrain the entire language model from scratch.\n",
        "\n",
        "<img src=\"./media/flamingo_arch.png\" width=500>\n",
        "\n",
        "The big concept here was that both the core vision and language models in Flamingo remain frozen; only the new cross-modal attention layers (and a small Perceiver Resampler module that compresses visual features are trained to achieve this. This makes training more efficient and avoids overfitting or forgetting the language model’s knowledge and prior text generation capability. This also allows for few-shot learning, specifically able to **generate relevant captions or answers about an image given just a few examples**. Without explicit fine-tuning, researchers proved that Flamingo could look at an image and answer complex questions about it or produce a detailed caption, simply by being prompted with a couple of examples of image-question-answer pairs. This showed that we can **teach an LLM to “see” and talk about what it sees** in a flexible, general way. Flamingo demonstrated an improvement from passive image *understanding* to active *generation* of textual descriptions and answers based on images.\n",
        "\n",
        "### BLIP-2: Efficiency Through Modularity\n",
        "\n",
        "<img src=\"./media/blip_overview.png\" width=400>\n",
        "\n",
        "[*BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models*](https://arxiv.org/pdf/2301.12597)\n",
        "\n",
        "Researchers also sought to make training multimodal models more **efficient and modular**. **BLIP-2** is an approach that achieves efficiency by **bootstraping from off-the-shelf components**- combining a frozen image encoder (like a pre-trained ViT or CLIP model) and a frozen large language model with a lightweight **Querying Transformer (Q-Former)** in between. The Q-Former is a small transformer that plays the role of an intermediary or translator between the vision and language parts.\n",
        "\n",
        "Training BLIP-2 is done in two stages:\n",
        "1. **Vision-language representation learning:** In the first stage, the Q-Former is trained to interpret and extract useful information from the frozen image encoder. In theory, the Q-Former “queries” the image features – it has learnable query tokens that attend to the image encoder’s output and is trained (using paired images and text) to produce embeddings that align with textual meaning. During this stage, the image encoder stays frozen, and the Q-Former learns to produce a compact representation of the image that captures concepts relevant to captions or labels.\n",
        "2. **Vision-to-language generation learning:** In the second stage, BLIP-2 connects the Q-Former to the frozen language model. Now the goal is to enable the language model to generate text based on the Q-Former’s image-informed embeddings. The Q-Former’s output is fed into the language model (for example, as a prefix or set of tokens the LM can attend to), and training teaches the combined system to generate appropriate text (captions, answers, etc.) for a given image. During this stage, the large language model remains frozen (not updated), and we fine-tune only the Q-Former.\n",
        "\n",
        "<img src=\"./media/blip_p2.png\" width=800>\n",
        "\n",
        "This parameter efficient approach efficiently reduces the number of parameters that need to be learned from scratch. The only newly trained part is the Q-Former, which results in a much more **scalable and efficient** strategy for multimodal learning. The BLIP-2 approach outperformed DeepMind’s 80B parameter Flamingo on a zero-shot image QA benchmark, all while using **54× fewer trainable parameters**. Since it’s modular, one can swap in different image encoders or language models as needed, and just train a new Q-Former to connect them.\n",
        "\n",
        "### Modern Day Vision LLMs\n",
        "\n",
        "<img src=\"./media/vlm-structure.png\" width=500>\n",
        "\n",
        "[*Vision Language Models Explained*](https://huggingface.co/blog/vlms)\n",
        "\n",
        "The evolution from CLIP to Flamingo to BLIP-2 and beyond has paved the way for a new generation of **vision-language LLMs** that are more capable and efficient than ever. Modern vision-enabled LLMs, especially in the open-source community, have largely standardized around a **modular architecture**: a pre-trained image encoder, a multimodal projector module, and a pre-trained text decoder (language model). The image encoder (often a ViT or CLIP model) produces a representation of the image, the projector (which can be as simple as a few linear layers or a small transformer) maps this representation into the language model’s embedding space, and the language model generates the final response. This design means that most of the heavy-lifting is done by components that were pre-trained on vast amounts of unimodal data (images or text), and only a comparatively small part (the projector and some connecting layers) needs training for the multimodal task.\n",
        "\n",
        "Recent vision-LLMs focus on improving **alignment** with human intent and maintaining **computational efficiency**. A good example is [**LLaVA (Large Language and Vision Assistant)**](https://huggingface.co/llava-hf/llava-v1.6-34b-hf), which refines the BLIP-2 approach. LLaVA uses a CLIP ViT-L/14 as its image encoder and Vicuna (an LLM based on LLaMA) as the text decoder, bridged by a learned projection layer. To train it, the authors generated a synthetic instruction-following dataset of image captions via GPT-4 along with question-answer pairs about each image- simulating an interactive user asking about the image. They then trained LLaVA in two phases: first freezing the image encoder and language model and training the projector on this data (to align the modalities), and then fine-tuning the language model with the projector for better performance.\n",
        "\n",
        "<img src=\"./media/llava_ex.png\" width=500>\n",
        "\n",
        "Advancements like those in LLaVA have improved how well the visual and textual parts cooperate (better alignment) and have made training these models more accessible (better efficiency). As researchers refine these techniques, we’re moving closer to AI systems that can see as well as they can read and write, enabling more natural and powerful human-AI interactions and use cases. The integration of vision into LLMs was only the first step, but it has proven pivotal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ONW4u3Tz4FnP",
      "metadata": {
        "id": "ONW4u3Tz4FnP"
      },
      "source": [
        "---\n",
        "### Image LLM Example\n",
        "\n",
        "Let's use [Moondream](https://huggingface.co/vikhyatk/moondream2), a small but powerful vision language model based on the Phi language model trained by [Vik Korrapati](https://x.com/vikhyatk) and team.\n",
        "\n",
        "We'll pass in the image:\n",
        "\n",
        "<img src=\"./example_media/moondream_ex.jpeg\" width=250>\n",
        "\n",
        "and ask for a description!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e379f2-bbca-4798-8b60-5adb16d5d6fb",
      "metadata": {
        "id": "40e379f2-bbca-4798-8b60-5adb16d5d6fb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"vikhyatk/moondream2\",\n",
        "    revision=\"2025-01-09\",\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\": \"cuda\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auc97Xv52hw2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auc97Xv52hw2",
        "outputId": "6c8d4feb-0615-4e6b-dba0-dcaf92310507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The image features a ginger cat with a white chest and paws, standing upright and holding a piece of paper in its paws. The cat's eyes are wide open, and it appears to be looking directly at the camera. The paper the cat is holding has the name \"Adam Lucek\" written on it in black letters. The background of the image is a solid, light blue-green color.\n"
          ]
        }
      ],
      "source": [
        "image = Image.open(\"./example_media/moondream_ex.jpeg\")\n",
        "\n",
        "print(model.query(image, \"Can you describe what's going on in this image?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a5ee7b-f6a4-42a5-8adc-e77c469d5adb",
      "metadata": {
        "id": "a2a5ee7b-f6a4-42a5-8adc-e77c469d5adb"
      },
      "source": [
        "---\n",
        "# From Images to Videos With Temporal Understanding\n",
        "\n",
        "Moving from image understanding to video understanding introduces **time** as a new dimension. An image is a static snapshot, whereas a video is a sequence of frames capturing dynamic content over time. This added temporal dimension increases complexity: motion, timing, and temporal relationships can change the interpretation of a scene. The good news however, is that techniques from image vision can be extended to videos, but now they must account for sequential structure. Modern video understanding models build on image models by integrating this temporal modeling, ensuring that motion and frame-to-frame dependencies are learned alongside spatial content.\n",
        "\n",
        "### Video Transformer Architectures\n",
        "\n",
        "<img src=\"./media/timesformer.png\" width=600>\n",
        "\n",
        "[*Is Space-Time Attention All You Need for Video Understanding?*](https://arxiv.org/pdf/2102.05095)\n",
        "\n",
        "Just as **Vision Transformers** (ViTs) enabled image recognition by modeling an image as a sequence of patches with self-attention. **Video transformers** extend this idea to sequences of image frames. The difference is that a video contains many more patches (spatial patches across multiple frames), so naively applying self-attention over all space-time patches is computationally expensive. To handle this, video transformers introduce structures to capture temporal information efficiently by treating a video as a sequence of frame embeddings (or even patch embeddings per frame) and incorporate temporal position encodings in addition to spatial ones, so the model *knows* the order of frames.\n",
        "\n",
        "Early adaptations like TimeSformer adapt the standard ViT for videos by enabling attention both **within each frame** and **across frames**. It uses a novel *divided space-time attention* mechanism: within each transformer block, it applies self-attention over the temporal dimension separately from the spatial dimension which means the model first attends to tokens along the time axis (capturing how a particular patch location evolves over frames), and then attends to tokens within each frame (capturing spatial relationships). This was proven to effectively model full spatiotemporal relationships. In other words, separating “when” and “where” attention preserves the ability to learn complex motion patterns without the full cost of joint 3D attention. TimeSformer’s attention-only design achieved state-of-the-art results on action recognition benchmarks, proving again that pure transformers can effectively learn video representations and that image-transformer techniques can be successfully extended to handle the temporal dynamics of video.\n",
        "\n",
        "### VideoMAE: Masked Autoencoders for Video Pre-Training\n",
        "\n",
        "<img src=\"./media/videomae.png\" width=600>\n",
        "\n",
        "[*VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training*](https://arxiv.org/pdf/2203.12602)\n",
        "\n",
        "Self-supervised learning has become key in vision, with two major families of approaches: **contrastive learning** (e.g. matching different views of the same scene) and **masked prediction** (e.g. masking parts of input and learning to reconstruct them). **VideoMAE** applies the masked autoencoder strategy to video to advance self-supervised video learning. Inspired by the Masked Autoencoder (MAE) for images, VideoMAE processes a video as a sequence of “tokens” (patches across frames) and masks a large portion of them during training. The model (a ViT backbone) must reconstruct the missing patches, forcing it to learn meaningful spatiotemporal features from the remaining context. A key innovation is the use of an **extremely high masking ratio** – typically 90% of the video patches are masked out. By making the task very challenging, VideoMAE encourages the encoder to extract rich video representations to fill in the blanks. The high masking ratio works especially well for video because temporal continuity provides additional context that static images lack.\n",
        "\n",
        "### InternVideo2: Unified Video Foundation Models\n",
        "\n",
        "<img src=\"./media/internvideo.png\" width=600>\n",
        "\n",
        "[*InternVideo2: Scaling Foundation Models for Multimodal Video Understanding*](https://arxiv.org/pdf/2403.15377)\n",
        "\n",
        "**InternVideo2** is a recent *video foundation model* that begins to unify multiple training objectives to learn versatile video representations and connect to text generation. Its approach is organized into three progressive stages, each adding a new capability on top of the previous:\n",
        "- **Masked Video Modeling (Stage 1)** – First, the video encoder is trained with a masked video token reconstruction task (analogous to VideoMAE). By learning to predict or reconstruct masked portions of video clips, the model learns spatiotemporal structures. This provides the base of low- and mid-level video understanding (motion, objects, scene dynamics) through self-supervision.\n",
        "- **Crossmodal Contrastive Learning (Stage 2)** – Next, InternVideo2 aligns video representations with other modalities, notably text (and also audio when available). In this stage, the video encoder (initialized from Stage 1’s weights) is trained jointly with a text encoder to produce matching embeddings for corresponding video-text pairs. This **video-text contrastive learning** is similar to how CLIP aligned images with captions, now applied to videos and captions for high-level semantic awareness of video content. After this stage, InternVideo2 is able to now align visual understanding to human language.\n",
        "- **LLM Integration via QFormer (Stage 3)** – In the final stage, InternVideo2 connects with a Large Language Model to enable **reasoning and generation** based on video input via a **Q-Former** module (inspired by BLIP-2 style architectures) that connects the video encoder and the language model. The video encoder (from Stage 2) produces a set of token embeddings for a given video and the Q-Former then attends to these video tokens and outputs a condensed set of “query” features that encapsulate the video’s information. These query features are fed into an LLM as prompts or special tokens to generate captions, answers, or dialog responses about the video, guided by the video-derived queries without needing to retrain the large language model from scratch.\n",
        "\n",
        "This progressive training yields a 6-billion-parameter video encoder that achieved impressive results across a wide range of video tasks (classification, retrieval, and video dialogue).\n",
        "\n",
        "### Apollo: Unified Video Understanding in Large Multimodal Models\n",
        "<img src=\"./media/apollo_exp.png\" width=600>\n",
        "\n",
        "[*Apollo: An Exploration of Video Understanding in Large Multimodal Models*](https://arxiv.org/pdf/2412.10360)\n",
        "\n",
        "More recently from Meta, **Apollo** demonstrates several modern techniques to advance video understanding in large multimodal models (LMMs), integrating spatial and temporal innovations. Apollo combines two visual encoders, a **SigLIP encoder** specialized for spatial detail and the **InternVideo2 encoder** optimized for temporal dynamics. By combining these complementary encoders, Apollo achieves spatiotemporal understanding superior to either encoder individually.\n",
        "\n",
        "To handle extensive video content efficiently, Apollo uses a new learned **Perceiver Resampler**. This module condenses detailed frame-level features into a compact set of tokens by selectively attending to key visual elements. This downsampling approach enables the model to manage long videos without losing critical information, significantly outperforming prior pooling methods. Theyadopts an **fps-based frame sampling** strategy, selecting continuous frame snippets rather than uniformly spaced frames which maintains natural temporal order and captures authentic motion patterns, resulting in better action recognition and temporal understanding.\n",
        "\n",
        "And finally, Apollo emphasizes balanced multimodal training by integrating pure text and image-text data alongside video content. This combined balance was shown to preserve the model’s language abilities and prevent overfitting to visual tasks, so the existing linguistic skills remain intact.\n",
        "\n",
        "<img src=\"./media/apollo_training.png\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e444b62b-cc31-45a2-bbc9-2aa101dc2b9c",
      "metadata": {
        "id": "e444b62b-cc31-45a2-bbc9-2aa101dc2b9c"
      },
      "source": [
        "---\n",
        "### Video LLM Example\n",
        "\n",
        "For video processing we'll try out [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) and ask it to process a short video clip\n",
        "\n",
        "<video src=\"./example_media/bakery.mp4\" width=\"500\" controls></video>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# default processer\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "8f6bee9c70ff4a6ba3eb3ceb8d0d7f33",
            "afaa940b0c8a474d968f27018a0d5007",
            "e3f79fe5697e46c6aa82f1ee1f5ea240",
            "dc61b5d0b6de44448c8811180ca6d52b",
            "612d000aea8344ef9eec72d80864fd54",
            "9a6f5307f8124a7d9c264cdb6f5ca89c",
            "7c6347550e3b4ef7a015458152a77662",
            "41c1a6c195064631ae32a45cab5314df",
            "23f677d14bf04f1ebbc0d2759dae7e52",
            "8f1ddd8aa1904549b7a66a1c2ae1ed21",
            "af52b06566334a4b91c0111452d4d1fa"
          ]
        },
        "id": "kI3uoDHC3zDU",
        "outputId": "4d42d4a8-26a5-4ecd-e070-8711aac2f417"
      },
      "id": "kI3uoDHC3zDU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f6bee9c70ff4a6ba3eb3ceb8d0d7f33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Messages containing a local video path and a text query\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"video\",\n",
        "                \"video\": \"./example_media/bakery.mp4\",\n",
        "                \"max_pixels\": 360 * 420,\n",
        "                \"fps\": 1.0,\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"Describe what happens in this video.\"},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preparation for inference\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    fps=1.0,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "# Inference\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHv582w_1D44",
        "outputId": "206572e6-1ec2-42c9-ccc7-b52e4a4a6cef"
      },
      "id": "BHv582w_1D44",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "qwen-vl-utils using decord to read video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The video starts with a view of a street scene featuring a bakery named \"Bakken met Passie.\" The bakery has a blue storefront with large windows displaying various baked goods and pastries. People are walking by on the sidewalk, and a cyclist rides past. The camera then pans to the right, showing more of the street and the adjacent buildings. The video ends with a clear view of the bakery\\'s exterior.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78408dc8-7fac-492b-bdbe-9aa226c176e3",
      "metadata": {
        "id": "78408dc8-7fac-492b-bdbe-9aa226c176e3"
      },
      "source": [
        "---\n",
        "# Past Vision to Hearing - Audio Understanding in LLMs\n",
        "\n",
        "<img src=\"./media/spectrogram.png\" width=600>\n",
        "\n",
        "While text and vision have been a focus, a key frontier is **auditory modality**. Audio understanding (from either speech or general acoustics) enables AI models the ability to grasp information conveyed in spoken language, environmental sounds, and music. Incorporating hearing lets an AI interpret context like tone of voice, background noises, or non-verbal cues that pure text or vision might miss. Enabling LLMs with audio comprehension, while overlooked, is important for real-world applications complementing visual understanding and moving toward truly multimodal intelligence.\n",
        "\n",
        "### Speech and General Acoustics\n",
        "\n",
        "<img src=\"./media/wave2vec.png\" width=600>\n",
        "\n",
        "[*Wave2Vec: Unsupervised Pre-Training for Speech Recognition*](https://arxiv.org/pdf/1904.05862)\n",
        "\n",
        "Early **speech models** set the foundation for audio-enabled LLMs by converting raw waveforms into meaningful representations. For example, **Wav2Vec 2.0** introduced a *self-supervised* framework that learns latent speech features directly from raw audio, masking portions of the waveform’s latent encoding and predicting them via a contrastive objective (think back to CLIP!). This approach yields embeddings that preserve audio-specific nuances (e.g. **tone** or speaker identity) without requiring transcripts.\n",
        "\n",
        "<img src=\"./media/whisper.png\" width=400>\n",
        "\n",
        "[*Robust Speech Recognition via Large-Scale Weak Supervision*](https://cdn.openai.com/papers/whisper.pdf)\n",
        "\n",
        "Jumping forward on the supervised side, OpenAI’s **Whisper** shows how scaling up speech recognition: it’s an encoder-decoder Transformer ASR model trained on *680,000 hours* of multilingual data, achieving remarkable robustness to accents, background noise, and technical language. Whisper processes audio by converting 30-second clips into a log-Mel spectrogram, feeding it into a Transformer encoder, and then decoding text along with special tokens for tasks like language identification, timestamps, and translation. Together, these speech models (and others in general acoustics) demonstrated how raw waveforms can be transformed into linguistic or semantic content – the first step for LLMs to ingest and reason about audio.\n",
        "\n",
        "### Audio Spectrogram Transformer (AST)\n",
        "\n",
        "<img src=\"./media/AST.png\" width=500>\n",
        "\n",
        "[*AST: Audio Spectrogram Transformer*](https://arxiv.org/pdf/2104.01778)\n",
        "\n",
        "Similar to what we've seen with audio and video, the **Audio Spectrogram Transformer (AST)** applies the success of transformers to the audio domain as the first purely attention-based audio classification model. It works by treating an audio **spectrogram** (a time–frequency heatmap of sound) like an image, splitting it into patches (akin to 16×16 pixel patches in Vision Transformers) and feeding these into a Transformer encoder. Surprisingly, AST can leverage pretrained Vision Transformer weights- the authors adapted a ViT to accept single-channel spectrogram input by averaging the ViT’s RGB patch embedding filters into one, transferring visual knowledge to audio. This approach allows AST to capture long-range context in audio, once again proving that Transformers can learn powerful acoustic representations from spectrograms. ASTs success opened the door to using **ViT-like architectures** for general audio understanding tasks.\n",
        "\n",
        "### CLAP (Contrastive Language-Audio Pretraining)\n",
        "\n",
        "<img src=\"./media/CLAP.png\" width=600>\n",
        "\n",
        "[*Clap: Learning Audio Concepts From Natural Language Supervision*](https://arxiv.org/pdf/2206.04769)\n",
        "\n",
        "Analogous to how CLIP aligned images with text, **CLAP** aligns audio and text representations through contrastive learning. CLAP consists of two encoders (one for audio, one for text) trained jointly on audio–caption pairs so that corresponding audio and text map to similar embeddings in a shared space. Given an audio clip and a textual description, a contrastive objective is used to **pull together** matching audio-text pairs and push apart mismatched ones. Despite training on only ~128k audio/text pairs (far fewer than image-text datasets), this approach proved effective and enabled the same enhanced *zero-shot* classification performance on diverse audio tasks without any explicit class labels. By learning audio concepts from natural language supervision, CLAP demonstrates the same usefulness of language to supervise audio models.\n",
        "\n",
        "### Audio Flamingo\n",
        "\n",
        "<img src=\"./media/audioflamingo.png\" width=600>\n",
        "\n",
        "[*Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities*](https://arxiv.org/pdf/2402.01831v1)\n",
        "\n",
        "**Audio Flamingo** extends the idea of DeepMind’s Flamingo to the audio. It is an *audio-language model* that aims to combine pretrained audio encoders with LLMs to enable open-ended reasoning about audio inputs. Instead of simply transcribing audio to text, Audio Flamingo feeds raw audio features directly into a language model via the same cross-attention mechanism, similar to how the original Flamingo handled image features. An audio feature extractor (e.g. a waveform model or a CLAP-like spectrogram encoder) produces a sequence of audio tokens, which are then fused into the LLM’s layers using learned cross-attention *gating*. This design lets the model attend to sounds (including **non-speech audio** and paralinguistic cues) during text generation. As a result, Audio Flamingo can adapt to new audio tasks with *few-shot prompts* and supports multi-turn conversations about what it “hears”. Using much of the same techniques from the vision training in the first Flamingo model family, Audio Flamingo proves that an LLM can be taught to interpret and discuss audio content by plugging in an audio modality interface.\n",
        "\n",
        "### GAMA (Generalist Audio Model for All)\n",
        "\n",
        "<img src=\"./media/GAMA.png\" width=600>\n",
        "\n",
        "[*GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities*](https://arxiv.org/pdf/2406.11768)\n",
        "\n",
        "On step above is **GAMA**, a general-purpose large audio-language model with advanced audio understanding and reasoning abilities. Architecturally, GAMA integrates a pretrained LLM with a dedicated audio front-end, introducing an *Audio Q-Former* module that serves as a connection between the audio encoder and the language model. The audio encoder (which could be a CNN or Transformer) produces intermediate features and the Audio Q-Former then aggregates these multi-layer audio features into a concise representation that the LLM can attend to. By fine-tuning this combined model on a large-scale audio-language dataset, GAMA learns to ground textual reasoning in audio inputs (much like how vision-language models ground text in images). After further instruction tuning, GAMA was able to answer open-ended audio-related questions requiring multi-step reasoning, outperforming other audio-language models by a substantial margin. GAMA showcases an **audio-first foundation model** pulling inspiration from BLIP-2 Q-Former approaches and training curricula to equip an LLM with a broad and deep understanding of sound."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020fa5f7-6363-40d6-8b47-0da48eccc1e3",
      "metadata": {
        "id": "020fa5f7-6363-40d6-8b47-0da48eccc1e3"
      },
      "source": [
        "---\n",
        "# Audio LLM Example\n",
        "\n",
        "This time we'll use a specific audio model [Qwen2-Audio-7B-Instruct](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct) with the sound effect of a car starting:\n",
        "\n",
        "<audio src=\"./example_media/car_start.wav\" controls></audio>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "import librosa\n",
        "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
        "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "d41ac9c5cf9542809ad056f265ab0675",
            "bed4daaec05f4ec2a3d7efbfd583a4c1",
            "d42cee7a01b64ec396d055838c889508",
            "b3de1fd52b134b33a3bdb3cc0792ebf1",
            "a701d8183995487ca08f69ecd0f49e21",
            "d7c7bb6fc87b494c8680b9e5f298e29f",
            "f62565222399400db6a1656724f6e628",
            "f141d77c1df7416d97564721a3552c5d",
            "a282df5b536945d1ba3d8c1bf8a9f3e0",
            "51747b1409ca44fea948534ae5d6713e",
            "a05bd0eed51d49bd98732cc4e43b1f5d"
          ]
        },
        "id": "2p6_2nN56qKA",
        "outputId": "4ac44573-e305-407c-a276-5db5979df5d0"
      },
      "id": "2p6_2nN56qKA",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d41ac9c5cf9542809ad056f265ab0675"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9d42a71d-6fae-4e66-9cee-780b3571b9ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d42a71d-6fae-4e66-9cee-780b3571b9ee",
        "outputId": "a77bab5d-c547-4b66-8fe9-ba5e6b5f8707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
          ]
        }
      ],
      "source": [
        "audio_file_path = \"./example_media/car_start.wav\"\n",
        "\n",
        "conversation = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"audio\", \"audio_url\": audio_file_path},\n",
        "        {\"type\": \"text\", \"text\": \"Describe this sound in detail.\"},\n",
        "    ]},\n",
        "]\n",
        "\n",
        "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "# Load the audio file\n",
        "audio, _ = librosa.load(audio_file_path, sr=processor.feature_extractor.sampling_rate)\n",
        "\n",
        "# Process inputs with the audio\n",
        "inputs = processor(text=text, audios=[audio], return_tensors=\"pt\", padding=True)\n",
        "inputs.input_ids = inputs.input_ids.to(\"cuda\")\n",
        "\n",
        "generate_ids = model.generate(**inputs, max_length=512)\n",
        "generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n",
        "\n",
        "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2xXeJ_K7uUp",
        "outputId": "d085634b-2136-41ec-f615-e6935b16760e"
      },
      "id": "Z2xXeJ_K7uUp",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The audio contains the sound of an engine starting up and revving, followed by the sound of it idling loudly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2978f7ec-e8b1-45f3-9d53-158c28e27565",
      "metadata": {
        "id": "2978f7ec-e8b1-45f3-9d53-158c28e27565"
      },
      "source": [
        "---\n",
        "# Multi Modality and Non Text Outputs - The Future\n",
        "\n",
        "<img src=\"./media/llama3_diagram.png\" width=600>\n",
        "\n",
        "[*The Llama 3 Herd of Models*](https://arxiv.org/pdf/2407.21783)\n",
        "\n",
        "Language models initially focused solely on text understanding but have gradually expanded their capabilities to embrace and combine multiple modalities, mirroring aspects of human cognition more closely. This evolution has unfolded consistently following a three-step pattern:\n",
        "\n",
        "1. **Specialized Encoders**: Each modality—such as images, videos, or speech—has dedicated encoders like Vision Transformers for images, TimeSformer for videos, and AST for speech.\n",
        "2. **Representation Alignment**: Modality-specific representations are aligned with textual embeddings using contrastive learning techniques, including CLIP for images and CLAP for audio, establishing a shared embedding space.\n",
        "3. **Integration with LLMs**: Aligned representations integrate into large language models through specialized adapter layers, enabling multimodal generation and interaction.\n",
        "\n",
        "These, three primary architectural strategies have emerged to bridge modality-specific encoders with language models. Cross-attention layers, exemplified by systems like Flamingo and Llama 3's vision module, empower language models to dynamically attend to modality-specific information during generation. Alternatively, adapters and modules such as BLIP-2’s Q-Former serve as bottlenecks, filtering and forwarding only the most relevant multimodal information into the language model. Lastly, in some instances, particularly with speech modalities like in Llama 3’s speech system, encoder outputs can be directly integrated into the language model's token space, enabling multimodal comprehension and generation without intermediate adapters.\n",
        "\n",
        "<img src=\"./media/gemini_card.png\" width=600>\n",
        "\n",
        "The latest models can now support all three major modalities simultaneously - a model like Gemini-1.5-pro can process up to 7,200 images, 2 hours of video, and 19 hours of audio while maintaining strong reasoning capabilities across data types.\n",
        "\n",
        "The use cases this unlocks for advanced reasoning past text are plentiful, including image-guided question answering, automated video summarization, precise semantic search in multimedia databases, real-time audio transcription and sentiment analysis, context-aware robotics interactions, advanced OCR and document understanding, multimodal content generation, and more with seamless interaction across text, voice, and visual interfaces.\n",
        "\n",
        "I personally use these capabilities with a couple programs:\n",
        "\n",
        "<img src=\"./media/ppt2desc.png\" width=400>\n",
        "\n",
        "[ppt2desc](https://github.com/ALucek/ppt2desc) uses vision-language models to convert PowerPoint presentations into detailed textual descriptions, capturing both textual content and the visual relationships within slides.\n",
        "\n",
        "<img src=\"./media/niavs.png\" width=400>\n",
        "\n",
        "[NeedleInAVidStack](https://github.com/ALucek/NeedleInAVidStack) automates video content extraction by processing audio tracks using Google's Gemini AI models, enabling precise semantic searches across extensive video collections.\n",
        "\n",
        "<img src=\"./media/any2any.png\" width=600>\n",
        "\n",
        "[*NExT-GPT: Any-to-Any Multimodal LLM*](https://arxiv.org/pdf/2309.05519)\n",
        "\n",
        "The future of AI research extends multimodal input to embrace omnimodal capabilities- those that can not only process multiple input modalities but also generate across various output channels (images to audio, text to video, speech to 3D models, and beyond). As these technologies mature, we'll move toward AI systems that begin to also mirror human cognitive flexibility by translating between different forms of perception and expression, removing barriers between modalities and creating more intuitive and natural human-machine interfaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71271a04-6d47-497e-82b0-9e6a98a8d97b",
      "metadata": {
        "id": "71271a04-6d47-497e-82b0-9e6a98a8d97b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f6bee9c70ff4a6ba3eb3ceb8d0d7f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afaa940b0c8a474d968f27018a0d5007",
              "IPY_MODEL_e3f79fe5697e46c6aa82f1ee1f5ea240",
              "IPY_MODEL_dc61b5d0b6de44448c8811180ca6d52b"
            ],
            "layout": "IPY_MODEL_612d000aea8344ef9eec72d80864fd54"
          }
        },
        "afaa940b0c8a474d968f27018a0d5007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6f5307f8124a7d9c264cdb6f5ca89c",
            "placeholder": "​",
            "style": "IPY_MODEL_7c6347550e3b4ef7a015458152a77662",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e3f79fe5697e46c6aa82f1ee1f5ea240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41c1a6c195064631ae32a45cab5314df",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23f677d14bf04f1ebbc0d2759dae7e52",
            "value": 2
          }
        },
        "dc61b5d0b6de44448c8811180ca6d52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1ddd8aa1904549b7a66a1c2ae1ed21",
            "placeholder": "​",
            "style": "IPY_MODEL_af52b06566334a4b91c0111452d4d1fa",
            "value": " 2/2 [00:03&lt;00:00,  1.59s/it]"
          }
        },
        "612d000aea8344ef9eec72d80864fd54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6f5307f8124a7d9c264cdb6f5ca89c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c6347550e3b4ef7a015458152a77662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c1a6c195064631ae32a45cab5314df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f677d14bf04f1ebbc0d2759dae7e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f1ddd8aa1904549b7a66a1c2ae1ed21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af52b06566334a4b91c0111452d4d1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d41ac9c5cf9542809ad056f265ab0675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bed4daaec05f4ec2a3d7efbfd583a4c1",
              "IPY_MODEL_d42cee7a01b64ec396d055838c889508",
              "IPY_MODEL_b3de1fd52b134b33a3bdb3cc0792ebf1"
            ],
            "layout": "IPY_MODEL_a701d8183995487ca08f69ecd0f49e21"
          }
        },
        "bed4daaec05f4ec2a3d7efbfd583a4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c7bb6fc87b494c8680b9e5f298e29f",
            "placeholder": "​",
            "style": "IPY_MODEL_f62565222399400db6a1656724f6e628",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d42cee7a01b64ec396d055838c889508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f141d77c1df7416d97564721a3552c5d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a282df5b536945d1ba3d8c1bf8a9f3e0",
            "value": 5
          }
        },
        "b3de1fd52b134b33a3bdb3cc0792ebf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51747b1409ca44fea948534ae5d6713e",
            "placeholder": "​",
            "style": "IPY_MODEL_a05bd0eed51d49bd98732cc4e43b1f5d",
            "value": " 5/5 [00:39&lt;00:00,  4.67s/it]"
          }
        },
        "a701d8183995487ca08f69ecd0f49e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c7bb6fc87b494c8680b9e5f298e29f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62565222399400db6a1656724f6e628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f141d77c1df7416d97564721a3552c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a282df5b536945d1ba3d8c1bf8a9f3e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51747b1409ca44fea948534ae5d6713e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05bd0eed51d49bd98732cc4e43b1f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}